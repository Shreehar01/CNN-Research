{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMPS 320\n",
    "## Lab 5:  Variable Selection and Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import time\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the Lab, we will be using the Credit data set. The data contains records balance(average credit card debt for a number of individuals) as well as several quantitative predictors: age, cards (number of credit cards), education (years of education), income (in thousands of dollars), limit (credit limit), and rating (credit rating)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>AtBat</th>\n",
       "      <th>Hits</th>\n",
       "      <th>HmRun</th>\n",
       "      <th>Runs</th>\n",
       "      <th>RBI</th>\n",
       "      <th>Walks</th>\n",
       "      <th>Years</th>\n",
       "      <th>CAtBat</th>\n",
       "      <th>CHits</th>\n",
       "      <th>...</th>\n",
       "      <th>CRuns</th>\n",
       "      <th>CRBI</th>\n",
       "      <th>CWalks</th>\n",
       "      <th>League</th>\n",
       "      <th>Division</th>\n",
       "      <th>PutOuts</th>\n",
       "      <th>Assists</th>\n",
       "      <th>Errors</th>\n",
       "      <th>Salary</th>\n",
       "      <th>NewLeague</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-Andy Allanson</td>\n",
       "      <td>293</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>293</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>446</td>\n",
       "      <td>33</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-Alan Ashby</td>\n",
       "      <td>315</td>\n",
       "      <td>81</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>14</td>\n",
       "      <td>3449</td>\n",
       "      <td>835</td>\n",
       "      <td>...</td>\n",
       "      <td>321</td>\n",
       "      <td>414</td>\n",
       "      <td>375</td>\n",
       "      <td>N</td>\n",
       "      <td>W</td>\n",
       "      <td>632</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>475.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-Alvin Davis</td>\n",
       "      <td>479</td>\n",
       "      <td>130</td>\n",
       "      <td>18</td>\n",
       "      <td>66</td>\n",
       "      <td>72</td>\n",
       "      <td>76</td>\n",
       "      <td>3</td>\n",
       "      <td>1624</td>\n",
       "      <td>457</td>\n",
       "      <td>...</td>\n",
       "      <td>224</td>\n",
       "      <td>266</td>\n",
       "      <td>263</td>\n",
       "      <td>A</td>\n",
       "      <td>W</td>\n",
       "      <td>880</td>\n",
       "      <td>82</td>\n",
       "      <td>14</td>\n",
       "      <td>480.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-Andre Dawson</td>\n",
       "      <td>496</td>\n",
       "      <td>141</td>\n",
       "      <td>20</td>\n",
       "      <td>65</td>\n",
       "      <td>78</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>5628</td>\n",
       "      <td>1575</td>\n",
       "      <td>...</td>\n",
       "      <td>828</td>\n",
       "      <td>838</td>\n",
       "      <td>354</td>\n",
       "      <td>N</td>\n",
       "      <td>E</td>\n",
       "      <td>200</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>500.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-Andres Galarraga</td>\n",
       "      <td>321</td>\n",
       "      <td>87</td>\n",
       "      <td>10</td>\n",
       "      <td>39</td>\n",
       "      <td>42</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>396</td>\n",
       "      <td>101</td>\n",
       "      <td>...</td>\n",
       "      <td>48</td>\n",
       "      <td>46</td>\n",
       "      <td>33</td>\n",
       "      <td>N</td>\n",
       "      <td>E</td>\n",
       "      <td>805</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>91.5</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Player  AtBat  Hits  HmRun  Runs  RBI  Walks  Years  CAtBat  \\\n",
       "0     -Andy Allanson    293    66      1    30   29     14      1     293   \n",
       "1        -Alan Ashby    315    81      7    24   38     39     14    3449   \n",
       "2       -Alvin Davis    479   130     18    66   72     76      3    1624   \n",
       "3      -Andre Dawson    496   141     20    65   78     37     11    5628   \n",
       "4  -Andres Galarraga    321    87     10    39   42     30      2     396   \n",
       "\n",
       "   CHits  ...  CRuns  CRBI  CWalks  League Division PutOuts  Assists  Errors  \\\n",
       "0     66  ...     30    29      14       A        E     446       33      20   \n",
       "1    835  ...    321   414     375       N        W     632       43      10   \n",
       "2    457  ...    224   266     263       A        W     880       82      14   \n",
       "3   1575  ...    828   838     354       N        E     200       11       3   \n",
       "4    101  ...     48    46      33       N        E     805       40       4   \n",
       "\n",
       "   Salary  NewLeague  \n",
       "0     NaN          A  \n",
       "1   475.0          N  \n",
       "2   480.0          A  \n",
       "3   500.0          N  \n",
       "4    91.5          N  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "hitters_df = pd.read_csv('Hitters.csv')\n",
    "hitters_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we note that the Salary variable is missing for some of the players. The isnull() function can be used to identify the missing observations. It returns a vector of the same length as the input vector, with a TRUE value for any elements that are missing, and a FALSE value for non-missing elements. The sum() function can then be used to count all of the missing elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values: 59\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of null values:\", hitters_df[\"Salary\"].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that Salary is missing for 59 players. The dropna() function removes all of the rows that have missing values in any variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of original data: (322, 21)\n",
      "Dimensions of modified data: (263, 20)\n",
      "Number of null values: 0\n"
     ]
    }
   ],
   "source": [
    "# Print the dimensions of the original Hitters data (322 rows x 20 columns)\n",
    "print(\"Dimensions of original data:\", hitters_df.shape)\n",
    "\n",
    "# Drop any rows the contain missing values, along with the player names\n",
    "hitters_df_clean = hitters_df.dropna().drop('Player', axis=1)\n",
    "\n",
    "# Print the dimensions of the modified Hitters data (263 rows x 20 columns)\n",
    "print(\"Dimensions of modified data:\", hitters_df_clean.shape)\n",
    "\n",
    "# One last check: should return 0\n",
    "print(\"Number of null values:\", hitters_df_clean[\"Salary\"].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of our predictors are categorical, so we'll want to clean those up as well. We'll ask pandas to generate dummy variables for them, separate out the response variable, and stick everything back together again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(hitters_df_clean[['League', 'Division', 'NewLeague']])\n",
    "\n",
    "y = hitters_df_clean.Salary\n",
    "\n",
    "# Drop the column with the independent variable (Salary), and columns for which we created dummy variables\n",
    "X_ = hitters_df_clean.drop(['Salary', 'League', 'Division', 'NewLeague'], axis=1).astype('float64')\n",
    "\n",
    "# Define the feature set X.\n",
    "X = pd.concat([X_, dummies[['League_N', 'Division_W', 'NewLeague_N']]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AtBat</th>\n",
       "      <th>Hits</th>\n",
       "      <th>HmRun</th>\n",
       "      <th>Runs</th>\n",
       "      <th>RBI</th>\n",
       "      <th>Walks</th>\n",
       "      <th>Years</th>\n",
       "      <th>CAtBat</th>\n",
       "      <th>CHits</th>\n",
       "      <th>CHmRun</th>\n",
       "      <th>CRuns</th>\n",
       "      <th>CRBI</th>\n",
       "      <th>CWalks</th>\n",
       "      <th>PutOuts</th>\n",
       "      <th>Assists</th>\n",
       "      <th>Errors</th>\n",
       "      <th>League_N</th>\n",
       "      <th>Division_W</th>\n",
       "      <th>NewLeague_N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>315.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3449.0</td>\n",
       "      <td>835.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>321.0</td>\n",
       "      <td>414.0</td>\n",
       "      <td>375.0</td>\n",
       "      <td>632.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>479.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1624.0</td>\n",
       "      <td>457.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>266.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>496.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5628.0</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>828.0</td>\n",
       "      <td>838.0</td>\n",
       "      <td>354.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>321.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>396.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>805.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>594.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4408.0</td>\n",
       "      <td>1133.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>501.0</td>\n",
       "      <td>336.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>421.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>497.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2703.0</td>\n",
       "      <td>806.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>379.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>492.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5511.0</td>\n",
       "      <td>1511.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>897.0</td>\n",
       "      <td>451.0</td>\n",
       "      <td>875.0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>381.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>475.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>433.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>573.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3198.0</td>\n",
       "      <td>857.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>470.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>1314.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>631.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4908.0</td>\n",
       "      <td>1457.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>775.0</td>\n",
       "      <td>357.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>408.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>263 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     AtBat   Hits  HmRun  Runs   RBI  Walks  Years  CAtBat   CHits  CHmRun  \\\n",
       "1    315.0   81.0    7.0  24.0  38.0   39.0   14.0  3449.0   835.0    69.0   \n",
       "2    479.0  130.0   18.0  66.0  72.0   76.0    3.0  1624.0   457.0    63.0   \n",
       "3    496.0  141.0   20.0  65.0  78.0   37.0   11.0  5628.0  1575.0   225.0   \n",
       "4    321.0   87.0   10.0  39.0  42.0   30.0    2.0   396.0   101.0    12.0   \n",
       "5    594.0  169.0    4.0  74.0  51.0   35.0   11.0  4408.0  1133.0    19.0   \n",
       "..     ...    ...    ...   ...   ...    ...    ...     ...     ...     ...   \n",
       "317  497.0  127.0    7.0  65.0  48.0   37.0    5.0  2703.0   806.0    32.0   \n",
       "318  492.0  136.0    5.0  76.0  50.0   94.0   12.0  5511.0  1511.0    39.0   \n",
       "319  475.0  126.0    3.0  61.0  43.0   52.0    6.0  1700.0   433.0     7.0   \n",
       "320  573.0  144.0    9.0  85.0  60.0   78.0    8.0  3198.0   857.0    97.0   \n",
       "321  631.0  170.0    9.0  77.0  44.0   31.0   11.0  4908.0  1457.0    30.0   \n",
       "\n",
       "     CRuns   CRBI  CWalks  PutOuts  Assists  Errors  League_N  Division_W  \\\n",
       "1    321.0  414.0   375.0    632.0     43.0    10.0         1           1   \n",
       "2    224.0  266.0   263.0    880.0     82.0    14.0         0           1   \n",
       "3    828.0  838.0   354.0    200.0     11.0     3.0         1           0   \n",
       "4     48.0   46.0    33.0    805.0     40.0     4.0         1           0   \n",
       "5    501.0  336.0   194.0    282.0    421.0    25.0         0           1   \n",
       "..     ...    ...     ...      ...      ...     ...       ...         ...   \n",
       "317  379.0  311.0   138.0    325.0      9.0     3.0         1           0   \n",
       "318  897.0  451.0   875.0    313.0    381.0    20.0         0           0   \n",
       "319  217.0   93.0   146.0     37.0    113.0     7.0         0           1   \n",
       "320  470.0  420.0   332.0   1314.0    131.0    12.0         0           0   \n",
       "321  775.0  357.0   249.0    408.0      4.0     3.0         0           1   \n",
       "\n",
       "     NewLeague_N  \n",
       "1              1  \n",
       "2              0  \n",
       "3              1  \n",
       "4              1  \n",
       "5              0  \n",
       "..           ...  \n",
       "317            1  \n",
       "318            0  \n",
       "319            0  \n",
       "320            0  \n",
       "321            0  \n",
       "\n",
       "[263 rows x 19 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Subset Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processSubset(feature_set):\n",
    "    # Fit model on feature_set and calculate RSS\n",
    "    model = sm.OLS(y,X[list(feature_set)])\n",
    "    regr = model.fit()\n",
    "    RSS = ((regr.predict(X[list(feature_set)]) - y) ** 2).sum()\n",
    "    return {\"model\":regr, \"RSS\":RSS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBest(k):\n",
    "    \n",
    "    tic = time.time()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for combo in itertools.combinations(X.columns, k):\n",
    "        results.append(processSubset(combo))\n",
    "    \n",
    "    # Wrap everything up in a nice dataframe\n",
    "    models = pd.DataFrame(results)\n",
    "    \n",
    "    # Choose the model with the highest RSS\n",
    "    best_model = models.loc[models['RSS'].argmin()]\n",
    "    \n",
    "    toc = time.time()\n",
    "    print(\"Processed\", models.shape[0], \"models on\", k, \"predictors in\", (toc-tic), \"seconds.\")\n",
    "    \n",
    "    # Return the best model, along with some other useful information about the model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a DataFrame containing the best model that we generated, along with some extra information about the model. Now we want to call that function for each number of predictors  k :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 19 models on 1 predictors in 0.05399775505065918 seconds.\n",
      "Processed 171 models on 2 predictors in 0.3759160041809082 seconds.\n",
      "Processed 969 models on 3 predictors in 2.21604061126709 seconds.\n",
      "Processed 3876 models on 4 predictors in 8.661441087722778 seconds.\n",
      "Processed 11628 models on 5 predictors in 27.78290319442749 seconds.\n",
      "Processed 27132 models on 6 predictors in 75.45013356208801 seconds.\n",
      "Processed 50388 models on 7 predictors in 162.26188564300537 seconds.\n",
      "Total elapsed time: 277.90842843055725 seconds.\n"
     ]
    }
   ],
   "source": [
    "models_best = pd.DataFrame(columns=[\"RSS\", \"model\"])\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(1,8):\n",
    "    models_best.loc[i] = getBest(i)\n",
    "\n",
    "toc = time.time()\n",
    "print(\"Total elapsed time:\", (toc-tic), \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have one big DataFrame that contains the best models we've generated along with their RSS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            RSS                                              model\n",
      "1  4.321393e+07  <statsmodels.regression.linear_model.Regressio...\n",
      "2  3.073305e+07  <statsmodels.regression.linear_model.Regressio...\n",
      "3  2.941071e+07  <statsmodels.regression.linear_model.Regressio...\n",
      "4  2.797678e+07  <statsmodels.regression.linear_model.Regressio...\n",
      "5  2.718780e+07  <statsmodels.regression.linear_model.Regressio...\n",
      "6  2.639772e+07  <statsmodels.regression.linear_model.Regressio...\n",
      "7  2.606413e+07  <statsmodels.regression.linear_model.Regressio...\n"
     ]
    }
   ],
   "source": [
    "print(models_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to access the details of each model, we can get a full rundown of a single model using the summary() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                 Salary   R-squared (uncentered):                   0.761\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.760\n",
      "Method:                 Least Squares   F-statistic:                              416.7\n",
      "Date:                Tue, 19 Oct 2021   Prob (F-statistic):                    5.80e-82\n",
      "Time:                        03:10:51   Log-Likelihood:                         -1907.6\n",
      "No. Observations:                 263   AIC:                                      3819.\n",
      "Df Residuals:                     261   BIC:                                      3826.\n",
      "Df Model:                           2                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Hits           2.9538      0.261     11.335      0.000       2.441       3.467\n",
      "CRBI           0.6788      0.066     10.295      0.000       0.549       0.809\n",
      "==============================================================================\n",
      "Omnibus:                      117.551   Durbin-Watson:                   1.933\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              654.612\n",
      "Skew:                           1.729   Prob(JB):                    7.12e-143\n",
      "Kurtosis:                       9.912   Cond. No.                         5.88\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "print(models_best.loc[2, \"model\"].summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output indicates that the best two-variable model contains only Hits and CRBI. To save time, we only generated results up to the best 7-variable model. You can use the functions we defined above to explore as many variables as are desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the best 19-variable model \n",
    "#print(getBest(19)[\"model\"].summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than letting the results of our call to the summary() function print to the screen, we can access just the parts we need using the model's attributes. For example, if we want the  $R^2$  value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7614950002332872"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_best.loc[2, \"model\"].rsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.664637\n",
       "2    0.761495\n",
       "3    0.771757\n",
       "4    0.782885\n",
       "5    0.789008\n",
       "6    0.795140\n",
       "7    0.797728\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gets the second element from each row ('model') and pulls out its rsquared attribute\n",
    "models_best.apply(lambda row: row[1].rsquared, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Stepwise Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(predictors):\n",
    "\n",
    "    # Pull out predictors we still need to process\n",
    "    remaining_predictors = [p for p in X.columns if p not in predictors]\n",
    "    \n",
    "    tic = time.time()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for p in remaining_predictors:\n",
    "        results.append(processSubset(predictors+[p]))\n",
    "    \n",
    "    # Wrap everything up in a nice dataframe\n",
    "    models = pd.DataFrame(results)\n",
    "    \n",
    "    # Choose the model with the highest RSS\n",
    "    best_model = models.loc[models['RSS'].argmin()]\n",
    "    \n",
    "    toc = time.time()\n",
    "    print(\"Processed \", models.shape[0], \"models on\", len(predictors)+1, \"predictors in\", (toc-tic), \"seconds.\")\n",
    "    \n",
    "    # Return the best model, along with some other useful information about the model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed  19 models on 1 predictors in 0.051030635833740234 seconds.\n",
      "Processed  18 models on 2 predictors in 0.04396557807922363 seconds.\n",
      "Processed  17 models on 3 predictors in 0.0409693717956543 seconds.\n",
      "Processed  16 models on 4 predictors in 0.04403233528137207 seconds.\n",
      "Processed  15 models on 5 predictors in 0.044031381607055664 seconds.\n",
      "Processed  14 models on 6 predictors in 0.04900479316711426 seconds.\n",
      "Processed  13 models on 7 predictors in 0.035036325454711914 seconds.\n",
      "Processed  12 models on 8 predictors in 0.03397107124328613 seconds.\n",
      "Processed  11 models on 9 predictors in 0.033035993576049805 seconds.\n",
      "Processed  10 models on 10 predictors in 0.030997753143310547 seconds.\n",
      "Processed  9 models on 11 predictors in 0.035001516342163086 seconds.\n",
      "Processed  8 models on 12 predictors in 0.031002044677734375 seconds.\n",
      "Processed  7 models on 13 predictors in 0.02402496337890625 seconds.\n",
      "Processed  6 models on 14 predictors in 0.021003007888793945 seconds.\n",
      "Processed  5 models on 15 predictors in 0.01703047752380371 seconds.\n",
      "Processed  4 models on 16 predictors in 0.014003515243530273 seconds.\n",
      "Processed  3 models on 17 predictors in 0.01199650764465332 seconds.\n",
      "Processed  2 models on 18 predictors in 0.009030818939208984 seconds.\n",
      "Processed  1 models on 19 predictors in 0.00400090217590332 seconds.\n",
      "Total elapsed time: 0.6398715972900391 seconds.\n"
     ]
    }
   ],
   "source": [
    "models_fwd = pd.DataFrame(columns=[\"RSS\", \"model\"])\n",
    "\n",
    "tic = time.time()\n",
    "predictors = []\n",
    "\n",
    "for i in range(1,len(X.columns)+1):    \n",
    "    models_fwd.loc[i] = forward(predictors)\n",
    "    predictors = models_fwd.loc[i][\"model\"].model.exog_names\n",
    "\n",
    "toc = time.time()\n",
    "print(\"Total elapsed time:\", (toc-tic), \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                 Salary   R-squared (uncentered):                   0.665\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.663\n",
      "Method:                 Least Squares   F-statistic:                              519.2\n",
      "Date:                Tue, 19 Oct 2021   Prob (F-statistic):                    4.20e-64\n",
      "Time:                        03:13:36   Log-Likelihood:                         -1952.4\n",
      "No. Observations:                 263   AIC:                                      3907.\n",
      "Df Residuals:                     262   BIC:                                      3910.\n",
      "Df Model:                           1                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Hits           4.8833      0.214     22.787      0.000       4.461       5.305\n",
      "==============================================================================\n",
      "Omnibus:                       90.075   Durbin-Watson:                   1.949\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              293.080\n",
      "Skew:                           1.469   Prob(JB):                     2.28e-64\n",
      "Kurtosis:                       7.256   Cond. No.                         1.00\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                 Salary   R-squared (uncentered):                   0.761\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.760\n",
      "Method:                 Least Squares   F-statistic:                              416.7\n",
      "Date:                Tue, 19 Oct 2021   Prob (F-statistic):                    5.80e-82\n",
      "Time:                        03:13:36   Log-Likelihood:                         -1907.6\n",
      "No. Observations:                 263   AIC:                                      3819.\n",
      "Df Residuals:                     261   BIC:                                      3826.\n",
      "Df Model:                           2                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Hits           2.9538      0.261     11.335      0.000       2.441       3.467\n",
      "CRBI           0.6788      0.066     10.295      0.000       0.549       0.809\n",
      "==============================================================================\n",
      "Omnibus:                      117.551   Durbin-Watson:                   1.933\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              654.612\n",
      "Skew:                           1.729   Prob(JB):                    7.12e-143\n",
      "Kurtosis:                       9.912   Cond. No.                         5.88\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "print(models_fwd.loc[1, \"model\"].summary())\n",
    "print(models_fwd.loc[2, \"model\"].summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that using forward stepwise selection, the best one-variable model contains only Hits, and the best two-variable model additionally includes CRBI. Let's see how the models stack up against best subset selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                 Salary   R-squared (uncentered):                   0.795\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.790\n",
      "Method:                 Least Squares   F-statistic:                              166.3\n",
      "Date:                Tue, 19 Oct 2021   Prob (F-statistic):                    1.79e-85\n",
      "Time:                        03:13:53   Log-Likelihood:                         -1887.6\n",
      "No. Observations:                 263   AIC:                                      3787.\n",
      "Df Residuals:                     257   BIC:                                      3809.\n",
      "Df Model:                           6                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "AtBat         -1.5488      0.477     -3.248      0.001      -2.488      -0.610\n",
      "Hits           7.0190      1.613      4.352      0.000       3.843      10.195\n",
      "Walks          3.7513      1.212      3.095      0.002       1.364       6.138\n",
      "CRBI           0.6544      0.064     10.218      0.000       0.528       0.781\n",
      "PutOuts        0.2703      0.075      3.614      0.000       0.123       0.418\n",
      "Division_W  -104.4513     37.661     -2.773      0.006    -178.615     -30.287\n",
      "==============================================================================\n",
      "Omnibus:                      106.414   Durbin-Watson:                   1.986\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              768.429\n",
      "Skew:                           1.433   Prob(JB):                    1.37e-167\n",
      "Kurtosis:                      10.869   Cond. No.                     1.29e+03\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.29e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                 Salary   R-squared (uncentered):                   0.795\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.790\n",
      "Method:                 Least Squares   F-statistic:                              166.3\n",
      "Date:                Tue, 19 Oct 2021   Prob (F-statistic):                    1.79e-85\n",
      "Time:                        03:13:53   Log-Likelihood:                         -1887.6\n",
      "No. Observations:                 263   AIC:                                      3787.\n",
      "Df Residuals:                     257   BIC:                                      3809.\n",
      "Df Model:                           6                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Hits           7.0190      1.613      4.352      0.000       3.843      10.195\n",
      "CRBI           0.6544      0.064     10.218      0.000       0.528       0.781\n",
      "Division_W  -104.4513     37.661     -2.773      0.006    -178.615     -30.287\n",
      "PutOuts        0.2703      0.075      3.614      0.000       0.123       0.418\n",
      "AtBat         -1.5488      0.477     -3.248      0.001      -2.488      -0.610\n",
      "Walks          3.7513      1.212      3.095      0.002       1.364       6.138\n",
      "==============================================================================\n",
      "Omnibus:                      106.414   Durbin-Watson:                   1.986\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              768.429\n",
      "Skew:                           1.433   Prob(JB):                    1.37e-167\n",
      "Kurtosis:                      10.869   Cond. No.                     1.29e+03\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.29e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "print(models_best.loc[6, \"model\"].summary())\n",
    "print(models_fwd.loc[6, \"model\"].summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Stepwise Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(predictors):\n",
    "    \n",
    "    tic = time.time()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for combo in itertools.combinations(predictors, len(predictors)-1):\n",
    "        results.append(processSubset(combo))\n",
    "    \n",
    "    # Wrap everything up in a nice dataframe\n",
    "    models = pd.DataFrame(results)\n",
    "    \n",
    "    # Choose the model with the highest RSS\n",
    "    best_model = models.loc[models['RSS'].argmin()]\n",
    "    \n",
    "    toc = time.time()\n",
    "    print(\"Processed \", models.shape[0], \"models on\", len(predictors)-1, \"predictors in\", (toc-tic), \"seconds.\")\n",
    "    \n",
    "    # Return the best model, along with some other useful information about the model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed  19 models on 18 predictors in 0.07004284858703613 seconds.\n",
      "Processed  18 models on 17 predictors in 0.053029775619506836 seconds.\n",
      "Processed  17 models on 16 predictors in 0.05797839164733887 seconds.\n",
      "Processed  16 models on 15 predictors in 0.056028127670288086 seconds.\n",
      "Processed  15 models on 14 predictors in 0.046970367431640625 seconds.\n",
      "Processed  14 models on 13 predictors in 0.043344974517822266 seconds.\n",
      "Processed  13 models on 12 predictors in 0.034803152084350586 seconds.\n",
      "Processed  12 models on 11 predictors in 0.04185366630554199 seconds.\n",
      "Processed  11 models on 10 predictors in 0.030995607376098633 seconds.\n",
      "Processed  10 models on 9 predictors in 0.028035402297973633 seconds.\n",
      "Processed  9 models on 8 predictors in 0.030973196029663086 seconds.\n",
      "Processed  8 models on 7 predictors in 0.022029399871826172 seconds.\n",
      "Processed  7 models on 6 predictors in 0.021032333374023438 seconds.\n",
      "Processed  6 models on 5 predictors in 0.021004438400268555 seconds.\n",
      "Processed  5 models on 4 predictors in 0.023009300231933594 seconds.\n",
      "Processed  4 models on 3 predictors in 0.02096414566040039 seconds.\n",
      "Processed  3 models on 2 predictors in 0.014976263046264648 seconds.\n",
      "Processed  2 models on 1 predictors in 0.01599884033203125 seconds.\n",
      "Total elapsed time: 0.6590752601623535 seconds.\n"
     ]
    }
   ],
   "source": [
    "models_bwd = pd.DataFrame(columns=[\"RSS\", \"model\"], index = range(1,len(X.columns)))\n",
    "\n",
    "tic = time.time()\n",
    "predictors = X.columns\n",
    "\n",
    "while(len(predictors) > 1):  \n",
    "    models_bwd.loc[len(predictors)-1] = backward(predictors)\n",
    "    predictors = models_bwd.loc[len(predictors)-1][\"model\"].model.exog_names\n",
    "\n",
    "toc = time.time()\n",
    "print(\"Total elapsed time:\", (toc-tic), \"seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this data, the best one-variable through six-variable models are each identical for best subset and forward selection. However, the best seven-variable models identified by forward stepwise selection, backward stepwise selection, and best subset selection are different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "Best Subset:\n",
      "------------\n",
      "Hits            1.680029\n",
      "Walks           3.399961\n",
      "CAtBat         -0.328835\n",
      "CHits           1.347017\n",
      "CHmRun          1.349373\n",
      "PutOuts         0.248166\n",
      "Division_W   -111.943760\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"------------\")\n",
    "print(\"Best Subset:\")\n",
    "print(\"------------\")\n",
    "print(models_best.loc[7, \"model\"].params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Foward Selection:\n",
      "-----------------\n",
      "Hits            7.277149\n",
      "CRBI            0.652415\n",
      "Division_W   -110.656338\n",
      "PutOuts         0.259787\n",
      "AtBat          -1.644651\n",
      "Walks           3.684324\n",
      "League_N       49.978410\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------\")\n",
    "print(\"Foward Selection:\")\n",
    "print(\"-----------------\")\n",
    "print(models_fwd.loc[7, \"model\"].params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "Backward Selection:\n",
      "-------------------\n",
      "AtBat         -1.601655\n",
      "Hits           6.148449\n",
      "Walks          5.866033\n",
      "CRuns          1.097453\n",
      "CWalks        -0.650614\n",
      "PutOuts        0.310125\n",
      "Division_W   -95.027171\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------------\")\n",
    "print(\"Backward Selection:\")\n",
    "print(\"-------------------\")\n",
    "print(models_bwd.loc[7, \"model\"].params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the sklearn package in order to perform ridge regression and the lasso. The main functions in this package that we care about are Ridge(), which can be used to fit ridge regression models, and Lasso() which will fit lasso models. They also have cross-validated counterparts: RidgeCV() and LassoCV()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ridge() function has an alpha argument ( λ , but with a different name!) that is used to tune the model. We'll generate an array of alpha values ranging from very big to very small, essentially covering the full range of scenarios from the null model containing only the intercept, to the least squares fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.00000000e+09, 3.78231664e+09, 2.86118383e+09, 2.16438064e+09,\n",
       "       1.63727458e+09, 1.23853818e+09, 9.36908711e+08, 7.08737081e+08,\n",
       "       5.36133611e+08, 4.05565415e+08, 3.06795364e+08, 2.32079442e+08,\n",
       "       1.75559587e+08, 1.32804389e+08, 1.00461650e+08, 7.59955541e+07,\n",
       "       5.74878498e+07, 4.34874501e+07, 3.28966612e+07, 2.48851178e+07,\n",
       "       1.88246790e+07, 1.42401793e+07, 1.07721735e+07, 8.14875417e+06,\n",
       "       6.16423370e+06, 4.66301673e+06, 3.52740116e+06, 2.66834962e+06,\n",
       "       2.01850863e+06, 1.52692775e+06, 1.15506485e+06, 8.73764200e+05,\n",
       "       6.60970574e+05, 5.00000000e+05, 3.78231664e+05, 2.86118383e+05,\n",
       "       2.16438064e+05, 1.63727458e+05, 1.23853818e+05, 9.36908711e+04,\n",
       "       7.08737081e+04, 5.36133611e+04, 4.05565415e+04, 3.06795364e+04,\n",
       "       2.32079442e+04, 1.75559587e+04, 1.32804389e+04, 1.00461650e+04,\n",
       "       7.59955541e+03, 5.74878498e+03, 4.34874501e+03, 3.28966612e+03,\n",
       "       2.48851178e+03, 1.88246790e+03, 1.42401793e+03, 1.07721735e+03,\n",
       "       8.14875417e+02, 6.16423370e+02, 4.66301673e+02, 3.52740116e+02,\n",
       "       2.66834962e+02, 2.01850863e+02, 1.52692775e+02, 1.15506485e+02,\n",
       "       8.73764200e+01, 6.60970574e+01, 5.00000000e+01, 3.78231664e+01,\n",
       "       2.86118383e+01, 2.16438064e+01, 1.63727458e+01, 1.23853818e+01,\n",
       "       9.36908711e+00, 7.08737081e+00, 5.36133611e+00, 4.05565415e+00,\n",
       "       3.06795364e+00, 2.32079442e+00, 1.75559587e+00, 1.32804389e+00,\n",
       "       1.00461650e+00, 7.59955541e-01, 5.74878498e-01, 4.34874501e-01,\n",
       "       3.28966612e-01, 2.48851178e-01, 1.88246790e-01, 1.42401793e-01,\n",
       "       1.07721735e-01, 8.14875417e-02, 6.16423370e-02, 4.66301673e-02,\n",
       "       3.52740116e-02, 2.66834962e-02, 2.01850863e-02, 1.52692775e-02,\n",
       "       1.15506485e-02, 8.73764200e-03, 6.60970574e-03, 5.00000000e-03])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphas = 10**np.linspace(10,-2,100)*0.5\n",
    "alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Associated with each alpha value is a vector of ridge regression coefficients, which we'll store in a matrix coefs. In this case, it is a  19×100  matrix, with 19 rows (one for each predictor) and 100 columns (one for each value of alpha). Remember that we'll want to standardize the variables so that they are on the same scale. To do this, we can use the normalize = True parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 19)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge = Ridge(normalize = True)\n",
    "coefs = []\n",
    "\n",
    "for a in alphas:\n",
    "    ridge.set_params(alpha = a)\n",
    "    ridge.fit(X, y)\n",
    "    coefs.append(ridge.coef_)\n",
    "    \n",
    "np.shape(coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect the coefficient estimates to be much smaller, in terms of  $l_2$  norm, when a large value of alpha is used, as compared to when a small value of alpha is used. Let's plot and find out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'weights')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = plt.gca()\n",
    "ax.plot(alphas, coefs)\n",
    "ax.set_xscale('log')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now split the samples into a training set and a test set in order to estimate the test error of ridge regression and the lasso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we fit a ridge regression model on the training set, and evaluate its MSE on the test set, using  λ=4 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AtBat           0.098658\n",
      "Hits            0.446094\n",
      "HmRun           1.412107\n",
      "Runs            0.660773\n",
      "RBI             0.843403\n",
      "Walks           1.008473\n",
      "Years           2.779882\n",
      "CAtBat          0.008244\n",
      "CHits           0.034149\n",
      "CHmRun          0.268634\n",
      "CRuns           0.070407\n",
      "CRBI            0.070060\n",
      "CWalks          0.082795\n",
      "PutOuts         0.104747\n",
      "Assists        -0.003739\n",
      "Errors          0.268363\n",
      "League_N        4.241051\n",
      "Division_W    -30.768885\n",
      "NewLeague_N     4.123474\n",
      "dtype: float64\n",
      "106216.52238005561\n"
     ]
    }
   ],
   "source": [
    "ridge2 = Ridge(alpha = 4, normalize = True)\n",
    "ridge2.fit(X_train, y_train)             # Fit a ridge regression on the training data\n",
    "pred2 = ridge2.predict(X_test)           # Use this model to predict the test data\n",
    "print(pd.Series(ridge2.coef_, index = X.columns)) # Print coefficients\n",
    "print(mean_squared_error(y_test, pred2))          # Calculate the test MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test MSE when alpha = 4 is 106216. Now let's see what happens if we use a huge value of alpha, say  $10^{10}$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AtBat          1.317464e-10\n",
      "Hits           4.647486e-10\n",
      "HmRun          2.079865e-09\n",
      "Runs           7.726175e-10\n",
      "RBI            9.390640e-10\n",
      "Walks          9.769219e-10\n",
      "Years          3.961442e-09\n",
      "CAtBat         1.060533e-11\n",
      "CHits          3.993605e-11\n",
      "CHmRun         2.959428e-10\n",
      "CRuns          8.245247e-11\n",
      "CRBI           7.795451e-11\n",
      "CWalks         9.894387e-11\n",
      "PutOuts        7.268991e-11\n",
      "Assists       -2.615885e-12\n",
      "Errors         2.084514e-10\n",
      "League_N      -2.501281e-09\n",
      "Division_W    -1.549951e-08\n",
      "NewLeague_N   -2.023196e-09\n",
      "dtype: float64\n",
      "172862.23580379886\n"
     ]
    }
   ],
   "source": [
    "ridge3 = Ridge(alpha = 10**10, normalize = True)\n",
    "ridge3.fit(X_train, y_train)             # Fit a ridge regression on the training data\n",
    "pred3 = ridge3.predict(X_test)           # Use this model to predict the test data\n",
    "print(pd.Series(ridge3.coef_, index = X.columns)) # Print coefficients\n",
    "print(mean_squared_error(y_test, pred3))          # Calculate the test MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This big penalty shrinks the coefficients to a very large degree, essentially reducing to a model containing just the intercept. This over-shrinking makes the model more biased, resulting in a higher MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so fitting a ridge regression model with alpha = 4 leads to a much lower test MSE than fitting a model with just an intercept. We now check whether there is any benefit to performing ridge regression with alpha = 4 instead of just performing least squares regression. Recall that least squares is simply ridge regression with alpha = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AtBat           -1.821115\n",
      "Hits             4.259156\n",
      "HmRun           -4.773401\n",
      "Runs            -0.038760\n",
      "RBI              3.984578\n",
      "Walks            3.470126\n",
      "Years            9.498236\n",
      "CAtBat          -0.605129\n",
      "CHits            2.174979\n",
      "CHmRun           2.979306\n",
      "CRuns            0.266356\n",
      "CRBI            -0.598456\n",
      "CWalks           0.171383\n",
      "PutOuts          0.421063\n",
      "Assists          0.464379\n",
      "Errors          -6.024576\n",
      "League_N       133.743163\n",
      "Division_W    -113.743875\n",
      "NewLeague_N    -81.927763\n",
      "dtype: float64\n",
      "116690.46856659211\n"
     ]
    }
   ],
   "source": [
    "ridge2 = Ridge(alpha = 0, normalize = True)\n",
    "ridge2.fit(X_train, y_train)             # Fit a ridge regression on the training data\n",
    "pred = ridge2.predict(X_test)            # Use this model to predict the test data\n",
    "print(pd.Series(ridge2.coef_, index = X.columns)) # Print coefficients\n",
    "print(mean_squared_error(y_test, pred))           # Calculate the test MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we are indeed improving over regular least-squares!\n",
    "\n",
    "Instead of arbitrarily choosing alpha  =4 , it would be better to use cross-validation to choose the tuning parameter alpha. We can do this using the cross-validated ridge regression function, RidgeCV(). By default, the function performs generalized cross-validation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5748784976988678"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridgecv = RidgeCV(alphas = alphas, scoring = 'neg_mean_squared_error', normalize = True)\n",
    "ridgecv.fit(X_train, y_train)\n",
    "ridgecv.alpha_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we see that the value of alpha that results in the smallest cross-validation error is 0.57. What is the test MSE associated with this value of alpha?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99825.6489629273"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge4 = Ridge(alpha = ridgecv.alpha_, normalize = True)\n",
    "ridge4.fit(X_train, y_train)\n",
    "mean_squared_error(y_test, ridge4.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This represents a further improvement over the test MSE that we got using alpha  =4 . Finally, we refit our ridge regression model on the full data set, using the value of alpha chosen by cross-validation, and examine the coefficient estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AtBat           0.055838\n",
       "Hits            0.934879\n",
       "HmRun           0.369048\n",
       "Runs            1.092480\n",
       "RBI             0.878259\n",
       "Walks           1.717770\n",
       "Years           0.783515\n",
       "CAtBat          0.011318\n",
       "CHits           0.061101\n",
       "CHmRun          0.428333\n",
       "CRuns           0.121418\n",
       "CRBI            0.129351\n",
       "CWalks          0.041990\n",
       "PutOuts         0.179957\n",
       "Assists         0.035737\n",
       "Errors         -1.597699\n",
       "League_N       24.774519\n",
       "Division_W    -85.948661\n",
       "NewLeague_N     8.336918\n",
       "dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge4.fit(X, y)\n",
    "pd.Series(ridge4.coef_, index = X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, none of the coefficients are exactly zero - ridge regression does not perform variable selection!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that ridge regression with a wise choice of alpha can outperform least squares as well as the null model on the Hitters data set. We now ask whether the lasso can yield either a more accurate or a more interpretable model than ridge regression. \n",
    "\n",
    "In order to fit a lasso model, we'll use the Lasso() function; however, this time we'll need to include the argument max_iter = 10000. Other than that change, we proceed just as we did in fitting a ridge model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'weights')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEKCAYAAAA1qaOTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXgc5Zn3++9dvWtfbcu2ZMn7vmGDzZ4BA1kAs2VYEiCQQ0JgkknOTF4y50wy52SZTDIJE0jCwJuEJRDAeEhwWAMEwmIbsMHGuy1bsi3LtlZLLanV6u563j+6bQtbsiW5peqW7891Fd39VFXrLrXpn6qeeqrEGINSSil1qiynC1BKKTU8aKAopZRKCg0UpZRSSaGBopRSKik0UJRSSiWFBopSSqmkcDtdgFOKiopMeXm502UopVRaWbt2bYMxprineadtoJSXl7NmzRqny1BKqbQiIrt7m6eHvJRSSiWFBopSSqmk0EBRSimVFBooSimlkkIDRSmlVFJooCillEoKDZR+ikRaqKt7xekylFIq5Wig9NOePb9hw8avsXXb97DtsNPlKKVUyjhtBzYOVEXF17FNF3v2/IbW1nXMmvlLAoFSp8tSSinH6R5KP1mWh0kTv8PsWf9NKLSH9z+4gpbW9U6XpZRSjtNAGaDi4iWcufA5PO481q27hdbWj50uSSmlHKWBcgoCgTLmz38CtzuPj9bdrKGilDqtaaCcIr9/NGfM/8ORUGlv3+l0SUop5QgNlCTw+0czf94TiHjYsPFuYrGQ0yUppdSQ00BJkkBgDDOm/5z29h1s2/7/OV2OUkoNOQ2UJCosPI/y8q+xf/8z7N//rNPlKKXUkNJASbLxFd8gL+8stm77Lh0dVU6Xo5RSQ0YDJclEXMyc8V+IuNix40dOl6OUUkNGA2UQ+HwjqCi/i4bGv9LY+LbT5Sil1JDQQBkkpaW3EPCXsaPyh9h21OlylFJq0GmgDBLL8jFx0j20t++gtvZpp8tRSqlBp4EyiIqLLiEv7yx2Vd1LJNLidDlKKTWoNFAGkYgwedL/SyTSTE3NY06Xo5RSg0oDZZBlZ0+noOA8avY9gW13OV2OUkoNGg2UIVBW+iW6uuo5WPei06UopdSg0UAZAgUF55GRMYG9ex/GGON0OUopNSg0UIaAiEVp6a0EgxtpaVnrdDlKKTUoNFCGSMmopbjdOezd+4jTpSil1KBwNFBE5HciUiciG7u1FYjIqyKyI/GY323ed0SkUkS2icil3drPEJENiXn3iYgM9bacjMuVwZjR11NX/wqhUI3T5SilVNI5vYfyCHDZMW33AK8bYyYBrydeIyLTgeuBGYl1fi0irsQ6DwB3AJMS07HvmRLGjv0iIm62bftXjLGdLkcppZLK0UAxxrwFNB3TfCXwaOL5o8DSbu1PGWPCxpgqoBI4U0RKgBxjzCoT7/F+rNs6KcXvH83kyf9KY9NbVFX/0ulylFIqqZzeQ+nJSGPMfoDE44hE+xhgb7flahJtYxLPj21PSWNG38CoUVdRVXUfjY1/c7ocpZRKmlQMlN701C9iTtB+/BuI3CEia0RkTX19fVKL6ysRYeqU75OVNYWNm75FR8duR+pQSqlkS8VAOZg4jEXisS7RXgOUdltuLFCbaB/bQ/txjDEPGWMWGGMWFBcXJ73wvnK5Asya+SvA5oM1SzlY95JjtSilVLKkYqCsAG5JPL8FeK5b+/Ui4hORCuKd7+8nDosFRWRR4uyum7utk7IyMspZuOBPZGRUsHHj3WzZ+i/EYiGny1JKqQFz+rThJ4FVwBQRqRGR24EfA0tEZAewJPEaY8wmYBmwGXgZuMsYE0u81Z3Ab4h31O8E0uJP/oyMcZwx/2nGjfsqtbXL+PCjL+hViZVSaUtO10uBLFiwwKxZs8bpMo6or/8LGzZ+g8zMCcyb+wheb5HTJSml1HFEZK0xZkFP81LxkNdpqbj4EubMfpCOjirWfngjneEDTpeklFL9ooGSQgoLz2funIcJhw+yft1txGIdTpeklFJ9poGSYvLzz2TWzPtoa9/Olq3/olcnVkqlDQ2UFFRYeAETxn+Lgwf/zN69DztdjlJK9YkGSooaN+5OiosvoXLnj2lqXuV0OUopdVIaKClKRJg+7acEAuPYsvnbOkZFKZXyNFBSmNudxdQpP6AzXMvu3Q85XY5SSp2QBkqKy88/i5EjPsfuPQ/qfVSUUilNAyUNTJx4D2Cxo/KHTpeilFK90kBJA35/CRXld1Ff/xcam95xuhyllOqRBkqaKCu7jUCgjKqqXzhdilJK9UgDJU1Ylo8RxZ+mtXUDsVjY6XKUUuo4GihpJDd3HsZECAY3OF2KUkodRwMljeTmzgOgpfUjhytRSqnjaaCkEa+3iIC/jJYWDRSlVOrRQEkzubnzaGn5SC8aqZRKORooaSYndx5dXXV0dtY6XYpSSn2CBkqaOdqP8qHDlSil1CdpoKSZrMypWFZA+1GUUilHAyXNWJabnJzZtGqgKKVSjAZKGsrNnUewbTOxWKfTpSil1BEaKGkoN2cuxkQJBjc6XYpSSh2hgZKGdICjUioVaaCkIR3gqJRKRRooaerwAEellEoVGihpKidnTnyAY/iA06UopRSggZK2cnLmANDaut7hSpRSKk4DJU1lZU1DxE1r68dOl6KUUkAKB4qIVIvIBhFZJyJrEm0FIvKqiOxIPOZ3W/47IlIpIttE5FLnKh8aLpePrKypuoeilEoZKRsoCZ8yxsw1xixIvL4HeN0YMwl4PfEaEZkOXA/MAC4Dfi0iLicKHko5OXNobd2AMbbTpSilVMoHyrGuBB5NPH8UWNqt/SljTNgYUwVUAmc6UN+QysmZTSzWRkdHldOlKKVUSgeKAf4iImtF5I5E20hjzH6AxOOIRPsYYG+3dWsSbcOadswrpVJJKgfKOcaY+cCngbtE5PwTLCs9tB13ByoRuUNE1ojImvr6+mTV6ZjMjPG4XFnaMa+USgkpGyjGmNrEYx3wR+KHsA6KSAlA4rEusXgNUNpt9bHAcXegMsY8ZIxZYIxZUFxcPJjlDwkRF9nZM3QPRSmVElIyUEQkU0SyDz8HLgE2AiuAWxKL3QI8l3i+ArheRHwiUgFMAt4f2qqdkZszh2DbFmw77HQpSqnTnNvpAnoxEvijiEC8xj8YY14WkQ+AZSJyO7AHuA7AGLNJRJYBm4EocJcxJuZM6UMrJ2cOxkQItm0lN9GnopRSTkjJQDHG7AKO+3Y0xjQCF/Wyzg+BHw5yaSknJ2c2EO+Y10BRSjkpJQ95qb7z+Urweou1H0Up5TgNlDQnIokBjnqml1LKWRoow0BW1hQ6Oqqx7S6nS1FKncY0UIaBQKAMsOns3Od0KUqp05gGyjAQCIwDIBTae5IllVJq8GigDAOBQHxMZyi0x+FKlFKnMw2UYcDnHYFl+TRQlFKOSslxKKp/RCwCgbJ4oAQPwu53IGsk5I6FnDHg8jhdolLqNKCBMkwE/KWEmjfAS2dBqPnoDJcXSs+C8RfCpCVQooMflVKDQwNlOGirI7BvG82e/ZiCCuSSJyHaCS17oW4rVL0Ff/1+fDrrq7Dk++D2Ol21UmqY0UBJV7YN1W/B2kdgy/MESjzEJmTQ9YUn8QVGHb98Wz28/TN47wHY9yFc9wjkDvtbxiilhpB2yqebtnp457/gl2fAY1fCzjfgzP+LwEU/BSDU1ctYlKxi+PSP4dqHoW4zPHgeHNg4hIUrpYY73UNxkm1DpAMiIYi0Q1c7hIMQboPOQ/G+kNAhaK+D4IH4VPsR2BEoOxsuuAemXwkeP4H2nVAdH4uSl3tG7z9z5tUwcmY8jJ64Fm5/FfJKe19eKaX6SAOln16ub2H5wSZmZAWYkRVgVnaAEt9J+iNiUTjwMex+F/ashkN7oK0uHhTGPvkP9eVC9kjIHgVnfQXm3wzFUz6xiN8/FpC+nTpcPBm+sBx+92l4/Bq47WXIKDj5ekopdQIaKP10KBplY1uI5+tbjrSV+r0szstkcW4WSwJdFLVWQ+OO+CGlAx/DgQ3Q1RZfuGA8FE6Mn22VNRL8ueAJxCdvFviy45M/FwL54M/rUwe6y+XD5xtJKLS7bxsycgZc/wQ8fjU8eQPc/Kd4DUopNUBizHG3Xj9+IZEJQI0xJiwiFwKzgceMMYcGub5Bs2DBArNmzZr+r9iwA/Z9SLCtiS2hCOu6PLxnclnlLaXJnY1lYixq+ZjP1b/J1c2rySsqh1GzoGwRlJ8b38sYJGs/vJHqKg9TmgPMKC1E3P54gM29EeI3Kzvepj/CM7fCorvgsh8NWm1KqeFBRNYaYxb0NK+veyj/AywQkYnAb4nfcvcPwGeSU2Ia2fYivPpdsoEzLTdnZhRxR85oTM5oNubN4sXMWTxfNIt/yZvH9y3h2lEF3DamiGlZAWzb8JeNB9hxMEhtSyd1rZ3kZ3qZUJzFhOJMppXkMDY/gPT25X8CTe1d/GrNxdy09zGmWuuJ7ffhjnUCBmJdsOBLPa844yqofgdW/xqmXQ7jFp/Sr0cpdfrq6x7Kh8aY+SLyz0CnMeZ+EfnIGDNv8EscHAPeQ2mrh84WyCyMH47q5ct/Y7CD3+1r4NmDzXTahgvdPlrXN7K5Jn6orCDTy4hsH03tXdQFj94PvjDTy6yxuSwsL+D8ScXMGJ2DZfX8MzojMT7c3czqXY08/t4eckN7eN37LZ7wfp6HvTfxyj+ei+cP18DeD+DOd6GgoudtCrfBA2eD5YKvvgPezHg/T9Xb8bEsh/t7xIoflvNmHj0sd3g6fHgukB/vj/Fm9b5XpJRKWyfaQ+lroLwH/Bfw/wCXG2OqRGSjMWZmcksdOgMOlH7a3Rrimy9u4sP1BzFui/POHst3z5nA5JyMI8u0dkaorGtjU20r6/ceYv3eQ+yoi/e5FGR6uXBKMZfPGc25E4uwjeH1LXU8s2Yv71Y20hWzsQTOqijkPzPvpaTyZV5b8gx3rIjw/aUz+eJUKx4Wo2bBLc+D1cuZ4jtejZ/1NWJ6fFBk066j8wIFkFkcfx7pSJyJFgQT633DXd744b2iyfFp9HyY+lnwZvS+jlIq5SUjUKYDXwVWGWOeFJEK4O+NMT9ObqlDZ7ADpbEtzO/ereKxlbsJhqN8du5ozNRc/tjcigGmZfq5pCiX0b74dbaE+B/0giBAW3sXlTUtVO5uYcuuZjq7YmT43bhFaA1FGJXj53OzSzh7YiELygvIkTDmZ5M5mBvBuu4J7v5jNrvq23jznz9F1uan4bmvwaU/gsV3HS0yFoFtL8G6J2DXm/EggfilWmZcFb9cS+HEnq8FZkw8XEKHEqc4Hz7NuRk6GiHUBC37oGF7vN8pGgJvNsy6BubfAmPmD9rvXik1eJLRh7LEGPP1wy8SeyihpFQ3jMRsw7uVDfzxo328tHE/4ajNp2eO4msXTmTmmFwA/lcozEv1LbzS2ML9uw9y0pOGx/mgdCRWQyexAyEEuOLCMv7znMn43d32Nj54FOlqZ++YXEZ27uVfPnMNS3/1Lg/9bSffWnIjbPkzvPpd2LA8vhcSyION/wPB/fELSJ5xK4y/AFb8I1geWHTniesSiR/68maefMS9bcOeVfDR72H90/HR/ePOhXO/CRMv0kNjSg0T/epDOabttOxDiRw4QGTf0dHobZ1RPt7XyvqaZj7cfYimjggZPhfnTCzmc3NKKM3PSOx6JL40RYD465Bt02kbDGCIP2LAmMNtYNuHI8cQsWHZgUZeawwyMdPPt8eXMC7DH3/Pp78AHj9rJgQpKr6Y8RO/wfee38I7u5r5452LyaEdPvgN0rgNadqOdDZiTfkUctaXYdIl4Er8bbH6AXj5nvjhsYrzBvz77VVnC3z0OKz8JQRrYdRsuPSHUHF+8n+WUirpBnzIS0RuAG4EzgXe7jYrG4gZYy5OZqFDaaCBUnnfA0R+fd8gVOQM8XiwMjKQjAysQADL78Nq3oyVmYk19e+wAn7E54+3Z2ZiZWVjZWXhys3FlZeHKz8Pd1ERVlZW/85Oi3bBhmfgzR9Dyx6YvhQu+T7klQ3exiqlTtmpHPJaCewHioCfdWsPAh8np7z08tfCGbx49h1ked2U5wcYX5jJzDE5TByVjddjxXcrgCNPjIlPxPc84q8T848N8098IcvxTw/PN4aWSIzf1tSx6VArP6p+kKJoO9Znfsbumkfp7NjH5An3YGI2976yBbfL4h8umhxfNRrFRCOYSAQTCmF3dGC3t2OHOrFDIexQB3Z0DJG63dhr38NEwe7sxHR2YiKRXn8vEgjgGTEC9+gSvGXj8JaV4q0Yj3/KZNyjRx8fNm4vzLspfimYlffD2z+H7a/AZ/8T5t6kh8GUSkN9OuQ1HA10D+X1P1ey6sVd5NoxxBggBsQwJoYQxe0Ft9fG7bFxe2K4PDEsVxTLFUEIg+nC5RZcHguXx43lssBI/AtXEo8IYlmIWCBWvJv+cLsYMF1EukJ0hUJkmg+4KPg6X5n8PcZ9sJPysWsont3A9mXzyMgu5sOsGfy5q4LvTmqifswoirKyGJeXy8SiAsbk5eH2+Y7/so+E4Bdz4pd3ueXPR5pNVxex9nbsYJBYSyuxQ83EmpuJNjQSPXiQSN1BIrW1RPbsJdZ89J4sVnY2/hkzyFy0iMyzF+OfMQNxuT75Mw/tjZ84UPUWzL4ePvsz8GX1+/NRSg2uZJzldTXwH8AIEickAcYYk5PMQofSQAPlrSef5IM/PTHAn+oF8cbDwRzuJTl2InF9ryO9KHTb7TnyPuLyUZbdxjWj3mI9M7n6nHvJNVF+2P4OvrwHoXEJ7XvL2N8U5MfRc7BGe2ifNfIT7zKheguLPn6XCcEm/Nk5BBJTZl4eWe07yap+Af/iL+OdtgRvIAOP34/H58fj9+MNZOD29H4nyFhrK+GdOwlv20bn1q2E1q0nvHUrAK6CAnKXLiXvumvxVXQbG2PH4K2fxg+DFU2Cm1dATskAf9dKqcGQjECpJD7+ZEuyi3PKQANlw8Pvw7aOI4ehTCJezeGOdwsQwViCuCxwWYjHhbhc2CLEjMEWwSa+b4NLwGOB28Lyu7B8bqyAC0/Ag8fvwu1xIZYkdlBsImGb9kNdhOobmb/rBiJR4emGn1NZlM0TF2YzJebiX/3/itfdweJFL7O8roVvP/EREozwwO1z8YeCVDe3sKGtkxUECFouJodaOa+umun7dhA91Ex7cxOhYOtJfxcujwdfRib+zKwjgZSZm0dmfgFZBYXkFBaRVzKGnKJiLJeLaGMj7atXE3z5FYJvvAHRKBmLFzHiG98gMHfu0Tfe9bf49cVGTINbXwCPv9+fk1JqcCQjUN41xpyT9MocNOBO+WXb6FpfH39hErliDu+2GcQc3YWzTDxfLAxWP/sEYsYQMdAFRC0h5rYwPhdkenAV+PE1vkpWw3Pk3vTvRItmsW/7IZ6qruc3pTAu2Iw/az/7ohNp83gp29tJ3eZGfn7ZdJaeX35k5H17LMaT+5v433vr2d3ZRZbL4ooRedxQUsjcgIdQUwOdHz5F13uP0tXWQsSdTUQCdImPCH7C+AjbXjptL6GYm1AXdIQidLS1f2JbLJeb/JLRjKiYwMiKiYyaMInCnDza/vw8Tb//PbGGBrIvvZQR3/om3nHj4ittfg6W3QxzvwBX/lL7VJRKEadyltfViacXAKOAPwFHrhNijHk2iXWeEhG5DPgF4AJ+c7JBlwMNlC2vvc+OVevIjWaRY7LIMzl43T7EYyFuC/FYWF4X4rUQnxvL70J8LiTgxvK5wOMCtxzZSzEGYjGbWGeMWCganzoi2KEodigKoSiEY7giNu6oTfeeB9sY2mwIGohkeXCVZvNhoYunrRBk7mSM3U7JvtlM2BjkfquNiqiL69xZTDxjBJMXjmRkRQ4igm0M77W089T+Jv5cf4iOmM2kDB83lRRyYWE2E11R3B89Bs27u923pS0xYr4V2g7GBzMmxPDSnjed1pzpNHvLORTLpeHAQQ5W7aS9uQmI792MmjCJ0glTGLt3P6Hlz0I0yohvf5v8L9wU79f56w/ih8A+/ZP4ZfuVUo47lUB5+ATva4wxt51qcckgIi5gO7AEqAE+AG4wxmzubZ2BBsqDb/+aX+564BNtuZLDCCmkiAJG2yMYExvJmMhIKjpLyAtlY3dGMZ0nuEyJgJXhxsry4spOTHk+3AV+XAV+PEUBrBxv/Mu/+mPCj9xFR/bFdIy6jsiBDqxDYdyR+HiVLtvQGDN0YBP2HyC/YgJ5Y0bwXHU9z+1u4OaRRZid7USiNqMn5bHoyvGUTMw7UkpbNMaKukM8sb+Rta0dAPgtYVpmgBKfhwyXRZbbRbbLIsftIsftosjrZrRlMyZcT1HzVqz9H8VvM7xvLXS1EbF9HMo9n9bcxdSbMupaojTX76GtoYrOtlrAJttbyOyaenL3V2GddQElP/gBOWMK4KkbYcdfYMn/Hx9sabl6+g0qpYbIKR/ySnUishj4N2PMpYnX3wEwxvx7b+sMNFAaQg3sbt1Nc2czzeFmGkON1HXUcbDjIPvb91MTrCEUPXoRgUJ/IVMLpzK3cC7z8+cy3T8FT9gV3wNpj2B3RLE7ItgdEWLBCHawi1hrF7FgmO7D6MXvxjsCClq+gkgI86U3cY08OkI92hImXHmI4OZGwnuCSFsX1kk+2pgx8b0kl4Urw40nx4c334c714crz0dDhout7hjrLJs10TCN0RjtMZv2WIzWaIxoD++fKUKFuCkLCyOaIuTsbiGwN4I3evSQlUc6CHhC+DPciE8ItlYRbN5MpKOWioZ2ptTWEfbl0T7tfEZcupiJrsdx7foLjD0TrvxV/AZhSilHJKMPpaeRfC3AGmPMc6dY3ykTkWuBy4wxX068/iJwljHm7t7WGWigtK9eTdsbbyBeL+L1IV5vfPCf348VCCCBAM3+KHutFirtg+zoqmFLsJLKQ5UYDG7LTaYnE4lFsCKhE9+xMfHRCCbRH2MYFY0yN3gO89r+jsnWeFxiJRa16QzsozljHe2BHXR56rGzGrG8EY726kji/br/98ggl27XEju6bPdiTuVPDxuDjY0t5hTfSSl1qg5tXsD133p0QOsm41pefmAq8Ezi9TXAJuB2EfmUMeYfB1RZ8vTUY3vct5aI3AHcAVBWNrAR2eHtOzi0/H8wXV0nHOhXmJjOSrzuGJHD9skZ7CiLQVYjlqedmO0lFvNj7MQpwomBj8a2489tA8aOnzyc6HPZ6/bwVv5atuZ8yFhjMc5lUZARJZAXwuWPH1aLhS3CQQ92kw9jfMSAoEQJi40RcBkB48YyLizjwmW74iFi5EjAWFh4xI1HPEceRaTHX3RPDBARm5AVodOKxs+GA9y2hcf0csXjHhw+6eETr5VSp6Srwzco79vXQJkI/J0xJgogIg8AfyHeZ7FhUCrrnxqgtNvrsUDtsQsZYx4CHoL4HspAflDw0sUcmF1INBYlFosQDXcSC4WwOzuIhTqIhdqxOzswoQ5M4lHCnUi4k0y7jU9HGrEPCc2hPNrCXowH8IJxx4eo4AXx2YjXIF4bl9fG8sVw+WK4fFHc/jDdzosg1mXR0eJhX22AlhYf3hof/kMWVXML2DUhwMFQ/HCcCxczGmdR0FnA9rzttOW1ce7Yc/lU2ac4Z/Q5ZHuzP7Gdu+rbWHLvW4zM9vHNJZO5ev5YXL3cl+WwtrY2qqur2blzJzt37qS1tRW3283UqVOZMWMG5eXlBAJ6m2Glhqu+BsoYIJP4YS4Sz0cbY2IiEu59tSHzATApcVn9fcD1xK9BlnQr3/w2RfkfxzvSMXgFxA1kJaaTODy6w0OQ/F6WsWMQi7qIRCwiEaEjKoTaXYRaPbRFoTEmNAL7DdR6YhiXC7fPTaSoE1MUwu/y0xlrgsb4IaySzBLm1c0j0Blg/jXz+aeSf6Isu+yE19568G+7cFvCc3efS3H2J/+aCYfDNDU10dDQQGNjIwcOHKC2tpbW1vjW+f1+xo8fz6RJk5g2bRp+v44jUep00NdA+QmwTkTeJH7U4XzgRyKSCbw2SLX1mTEmKiJ3A68QP234d8aYTYPxs4qKzqOx3pAYwXj00ii4AAvBBeJCcCHiRvAgdgwaqxFbYMRMxJMN4kXEC3gweDC2G4MXY3sAF6bb9b8ssfG7Y7itKJlum1HEL9FiiYXH5cHtcmNZFjFi7AruorGjkYJAASMyR1CcUYzX8rJq0ypKSkooqC+gtqmWg66DuN1u3G43Llf85xljsG2bxmAHm9Zv4e9Ls/lw5Zu0t7fT3t5OMBikpaWFcPiTf0MUFBRQVlbG6NGjKS0tZfTo0biOvbSKUmrY6/NZXiJSApxJPFDeN8Ycd0gpnQy0U/6jjz5i9erVdP+99fT8yGMsgmmtjfeFZJVgLPcnlj28l3Cix+5Td91DIBaLHXmMxWKfmGfbNgM9m8/j8ZCRkUFmZibZ2dnk5uaSk5NDfn4+RUVFFBQU4DnBJViUUsPLgDvlRWSqMWariBy+F8rexOMoERlljPkwmYWmg3nz5jFvXh9vA1P9Djz9RfDG4Kb/gdKFg1tcL5588kn27dvHN77xDYwxR0InFosRjUaJRqNHAisYjnHtg+9x4bQSfvL5+bjdfd2JVUqd7k72bfEt4mdF/ayHeQb4u6RXNFysfQRe+L8hvwJufBoKJzhSRigUorKykoULF/ZpT+L3r26nKeLmqxdN0zBRSvXLCb8xjDF3JB4/NTTlDAMtNfDS/4Ktz8PEi+Ga38Zvt+uQrVu3EovFmDlz5kmXbe2M8OjKai6ZPpLJI7NPurxSSnXXpz9BRSSD+N5KmTHmDhGZBEwxxjw/qNWlk1gEPvhN/PpTdgwu/jdY/A9Hb63rkI0bN5KXl8eYMSe57zvwi9d20NoZ4esXTRqCypRSw01fv+0eBtYCZyde1xAf5KiB0tWRuEf6fdCyN75X8tmfQX6505XR3t7Orl27OOecc056e97tB4M8srKa6xeWMXNM7hBVqJQaTvoaKBOMMX+fuMc8xpiQ9OsG4sPQgY3w8dOw7g/Q0QCli+CzP4dJS1LmUutbt27FGHPSw13GGP5txSayfG7++dIpQ1SdUmq46WugdIlIgMnfIvEAABGASURBVMRFMERkAt2Ha59ONq+AN/8d6jaD5YZJl8DZ/wDjzj75ukNs586d5OTkMHLkyBMu9+KGA6zc2cj3r5xBQaZ3iKpTSg03fQ2U7wEvA6Ui8gRwDnDrYBWV0mJd4MuOH9aafhVkFjpdUY9s26aqqorJkyef8HBXR1eUH7ywmeklOdx41rghrFApNdz0NVBuBl4AlgO7gG8YYxoGrapUNvMamHWt01Wc1MGDBwmFQowfP/6Ey/3HS1vZ39LJ/TfMO+m1upRS6kT60yl/LvGLQY4nfhmWt4wxvxi0ylJVivSPnMyuXbsAqKio6HWZlTsbeHTVbr50TjkLyguGqjSl1DDVp0AxxvxVRP4GLAQ+BXwVmEH8lrsqBVVVVVFUVEROTk6P89vDUb69/GPKCzP49qVTh7g6pdRw1NdxKK8Tv8LwKuBtYKExpm4wC1MDF41G2b17N3Pnzu11mR+/tJV9h0Is+8piAl69kKNS6tT19U5HHwNdwExgNjAzcdaXSkH79u0jEon02n/y8sYD/H71bm47p4KFeqhLKZUkfT3k9U0AEckCvkS8T2UUMDi3/VKnZNeuXYgI5eXlx817b1cjX3/qI+aW5umYE6VUUvX1kNfdwHnAGcBu4HfED32ddlasr+Xx1bvJ9LrI8LrJ8LrI8rvJ9nvI8bvJy/CSn+EhP9PLiGwfxdk+fO6hPaRUVVVFSUnJcXdH3HqglS8/tobS/AAP37oQv0cPdSmlkqevZ3kFgJ8Daw/fBvh019DWRUdXBx1dMdo6o7R1RentliP5GR7G5mdQWhCgND+DTJ8bj8vC4xK8bguvy8LrtvC5XQS8Fn63i4DXRaYvHljZfg/ZPjdWH07rDYfD1NTUcPbZnxxouam2hdse+YBMr5vHbj+LfB3AqJRKsr4e8vrpYBeSLq6YM5or5ow+rt22DcFwlJaOCE0dXTS3d1EX7ORga5gDrZ3UNIfYuj/Ia5vr6IrZ/f65lkBOwMPY/ABzxuYxtzSPGaNzKS0IkO0/eln6PXv2YNv2kdOF28NR7n11Ow+vrCY/w8MTXz6LMXna/aWUSj694UWSWJaQG/CQG/BQVphxwmVjtiESs+mK2XRFj07hqE0oEqMzEiMUidERjtEejtLaGaElFOFQR4SqhnZWrKvliff2HHm/vAwPE4qzWDy+kOLgdsSy2Nbm45mXtrJi3T5qWzq54cwy7rlsKrkZendFpdTg0EBxgMsSXJZrwH0Ytm3YWd/G9oNt7G3uoKa5g021rTzwt51c4NpJtvj42pPr8biEeaX53H/jPM4Yp2dzKaUGlwZKGrIsYdLIbCYdcxOsYGeEX92/FVf2SJ79zNlML8nRjnel1JDp6zgUlQZ8lqGzvZUzp1Uwvyxfw0QpNaQ0UIaR+vp6AIqLix2uRCl1OtJAGUYOB8qIESMcrkQpdTrSQBlG6urqcLlcFBRoB7xSauhpoAwjdXV1FBcXY1n6sSqlhp5+8wwjhwNFKaWcoIEyTHR2dtLa2qr9J0opx2igDBPaIa+UcpoGyjBRVxe/35kGilLKKSkXKCLybyKyT0TWJabPdJv3HRGpFJFtInJpt/YzRGRDYt59Imly4/ckqqurw+PxkJub63QpSqnTVMoFSsK9xpi5ielFABGZDlxP/F72lwG/FpHDQ8EfAO4AJiWmyxyo2VF6hpdSymnp9O1zJfCUMSZsjKkCKoEzRaQEyDHGrDLGGOAxYKmThTqhvr5eD3cppRyVqoFyt4h8LCK/E5H8RNsYYG+3ZWoSbWMSz49tP46I3CEia0RkzeFO7OGgo6ODtrY2DRSllKMcCRQReU1ENvYwXUn88NUEYC6wH/jZ4dV6eCtzgvbjG415yBizwBizYDiN1zjcIT+ctkkplX4cuXy9MebiviwnIv8beD7xsgYo7TZ7LFCbaB/bQ/tpQ8/wUkqlgpQ75JXoEznsKmBj4vkK4HoR8YlIBfHO9/eNMfuBoIgsSpzddTPw3JAW7bADBw7g9/vJyclxuhSl1GksFW+w9RMRmUv8sFU18BUAY8wmEVkGbAaiwF3GmFhinTuBR4AA8FJiOm1UV1dTVlbGaXi2tFIqhaRcoBhjvniCeT8EfthD+xpg5mDWlapaWlpoampi4cKFTpeilDrNpdwhL9U/1dXVAJSXlztah1JKaaCkuaqqKgKBACNHjnS6FKXUaU4DJc1VV1czbtw4HSGvlHKcfgulsebmZg4dOkRFRYXTpSillAZKOquqqgK0/0QplRo0UNJYdXU1GRkZOqBRKZUSNFDSlDGGqqoqysvLdfyJUiolaKCkqaamJoLBoPafKKVShgZKmtL+E6VUqtFASVPV1dVkZWVRVFTkdClKKQVooKSlw/0nFRUV2n+ilEoZGihpqL6+nvb2du0/UUqlFA2UNHS4/0QDRSmVSjRQ0lBVVRV5eXnk5+effGGllBoiGihpxrZtqqurde9EKZVyNFDSzIEDB+js7NRAUUqlHA2UNKPjT5RSqUoDJc1UVVVRVFSk949XSqUcDZQ0EovF2LNnjx7uUkqlJA2UNFJbW0tXV5cGilIqJWmgpBHtP1FKpTINlDRhjGHr1q2MGjWKjIwMp8tRSqnjaKCkid27d1NbW8uCBQucLkUppXqkgZImVq5cSUZGBnPmzHG6FKWU6pEGShqor69n+/btnHnmmXg8HqfLUUqpHmmgpIFVq1bhdrtZuHCh06UopVSvNFBSXDAYZP369cydO5fMzEyny1FKqV5poKS4999/n1gsxuLFi50uRSmlTsiRQBGR60Rkk4jYIrLgmHnfEZFKEdkmIpd2az9DRDYk5t0niVsViohPRJ5OtL8nIuVDuzWDp6WlhdWrVzNt2jQKCwudLkcppU7IqT2UjcDVwFvdG0VkOnA9MAO4DPi1iLgSsx8A7gAmJabLEu23A83GmInAvcB/DHr1Q+SVV17BGMOSJUucLkUppU7KkUAxxmwxxmzrYdaVwFPGmLAxpgqoBM4UkRIgxxizyhhjgMeApd3WeTTxfDlwkQyDG61XVlayefNmzjvvPAoKCpwuRymlTirV+lDGAHu7va5JtI1JPD+2/RPrGGOiQAvQ4/EhEblDRNaIyJr6+vokl5480WiUF198kYKCAs4++2yny1FKqT4ZtEARkddEZGMP05UnWq2HNnOC9hOtc3yjMQ8ZYxYYYxYUFxefeAMctHLlSpqamvjMZz6j406UUmnDPVhvbIy5eACr1QCl3V6PBWoT7WN7aO++To2IuIFcoGkAPzsl7NmzhzfffJPp06czceJEp8tRSqk+S7VDXiuA6xNnblUQ73x/3xizHwiKyKJE/8jNwHPd1rkl8fxa4K+Jfpa009rayrJly8jLy+Pyyy93uhyllOqXQdtDORERuQq4HygGXhCRdcaYS40xm0RkGbAZiAJ3GWNiidXuBB4BAsBLiQngt8DvRaSS+J7J9UO3JckTjUZ5+umn6erq4uabbyYQCDhdklJK9Yuk6R/zp2zBggVmzZo1TpcBgG3brFixgnXr1vH5z3+e6dOnO12SUkr1SETWGmN6vOx5qh3yOu10D5MLLrhAw0QplbYcOeSl4qLRKM8++yybN2/mwgsv5IILLnC6JKWUGjANFId0dXWxbNkyKisrueSSS3S8iVIq7WmgOCAUCvHkk0+yZ88eLr/8cs444wynS1JKqVOmgTLEgsEgjz/+OPX19Vx33XXMmDHD6ZKUUiopNFCGUEdHBw8//DDBYJCbbrqJCRMmOF2SUkoljQbKEDHGsGLFCg4dOsStt95KWVmZ0yUppVRS6WnDQ2TNmjVs3bqViy++WMNEKTUsaaAMgYMHD/LKK68wceJEFi1a5HQ5Sik1KDRQBlk4HGb58uX4fD6WLl2KZemvXCk1POm32yCKRCI89dRTNDQ0cNVVV5GVleV0SUopNWg0UAZJLBbjmWeeoaqqiqVLl+ql6JVSw54GyiCIxWI8++yzbN++nc997nPMmTPH6ZKUUmrQ6WnDSdbe3s7y5cupqqpiyZIlLFjQ40U5lVJq2NFASaLa2lqefvpp2traWLp0KXPnznW6JKWUGjIaKEkQjUZ57733eOONN8jIyOC2225jzJgxTpellFJDSgPlFFVVVfHCCy/Q0NDAlClTuPzyy/VsLqXUaUkDZQCMMezatYuVK1eyc+dO8vLyuOGGG5gyZYrTpSmllGM0UPqpsrKS1157jQMHDpCVlcVFF13EokWL8Hg8TpemlFKO0kDpp/b2dqLRKFdccQWzZ8/G7dZfoVJKgQZKv82aNYtZs2bpJVSUUuoYGij9pEGilFI9029HpZRSSaGBopRSKik0UJRSSiWFBopSSqmk0EBRSimVFBooSimlkkIDRSmlVFKIMcbpGhwhIvXAbqfrOIkioMHpIpJguGwH6LakquGyLemwHeOMMcU9zThtAyUdiMgaY0za36FruGwH6LakquGyLem+HXrISymlVFJooCillEoKDZTU9pDTBSTJcNkO0G1JVcNlW9J6O7QPRSmlVFLoHopSSqmk0EBRSimVFBooSimlkkIDJQ2JyHgR+a2ILHe6loFI9/q7E5FpIvLfIrJcRO50up5TISIXisjbie250Ol6BkpEzktsw29EZKXT9ZwKEZkuIstE5AERudbpek5GA2WIicjvRKRORDYe036ZiGwTkUoRuedE72GM2WWMuX1wK+2f/mxXKtbfXT+3ZYsx5qvA54GUG5DWz39vBmgD/EDNUNd6Iv38TN5OfCbPA486Ue+J9PMz+TRwvzHmTuDmIS+2v4wxOg3hBJwPzAc2dmtzATuB8YAXWA9MB2YR/5+i+zSi23rLnd6egWxXKtZ/KtsCXAGsBG50uvZT/PdmJeaPBJ5wuvYk/PtaBuQ4XfspfiYjgF8BPwXedbr2k026hzLEjDFvAU3HNJ8JVJr4X+5dwFPAlcaYDcaYzx0z1Q150X3Qn+0a8uL6qb/bYoxZYYw5G7hpaCs9uX7+e7MT85sB3xCWeVL9/UxEpAxoMca0Dm2lJ9fPz6TOGHMXcA+pf40vDZQUMQbY2+11TaKtRyJSKCL/DcwTke8MdnGnoMftSqP6u+ttWy4UkftE5EHgRWdK67fetuXqxHb8HvilI5X1z4n+v7kdeHjIKxq43j6TchF5CHiM+F5KSnM7XYACQHpo63XEqTGmEfjq4JWTND1uVxrV311v2/Im8ObQlnLKetuWZ4Fnh7qYU9Dr/zfGmO8NcS2nqrfPpBq4Y4hrGTDdQ0kNNUBpt9djgVqHakmm4bRdui2pZ7hsBwyTbdFASQ0fAJNEpEJEvMD1wAqHa0qG4bRdui2pZ7hsBwyTbdFAGWIi8iSwCpgiIjUicrsxJgrcDbwCbAGWGWM2OVlnfw2n7dJtST3DZTtgeG3LsfTikEoppZJC91CUUkolhQaKUkqppNBAUUoplRQaKEoppZJCA0UppVRSaKAopZRKCg0UpRwgItUiUnSqyyiVSjRQlFJKJYUGilKDTET+JCJrRWSTiNxxzLxyEdkqIo+KyMeJOz9mdFvkH0TkQxHZICJTE+ucKSIrReSjxOOUId0gpXqhgaLU4LvNGHMG8Ts6fl1ECo+ZPwV4yBgzG2gFvtZtXoMxZj7wAPBPibatwPnGmHnAd4EfDWr1SvWRBopSg+/rIrIeWE38irKTjpm/1xjzbuL548C53eYdvpz8WqA88TwXeCZxC9l7gRmDUbRS/aWBotQgEpELgYuBxcaYOcBHxO/Z3t2xF9Tr/jqceIxx9P5F3wfeMMbMBC7v4f2UcoQGilKDKxdoNsZ0JPpAFvWwTJmILE48vwF4pw/vuS/x/NakVKlUEmigKDW4XgbcIvIx8T2L1T0sswW4JbFMAfH+khP5CfDvIvIu4EpmsUqdCr18vVIOEpFy4PnE4Sul0pruoSillEoK3UNRSimVFLqHopRSKik0UJRSSiWFBopSSqmk0EBRSimVFBooSimlkkIDRSmlVFL8H0B06QfHOypKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lasso = Lasso(max_iter = 10000, normalize = True)\n",
    "coefs = []\n",
    "\n",
    "for a in alphas:\n",
    "    lasso.set_params(alpha=a)\n",
    "    lasso.fit(scale(X_train), y_train)\n",
    "    coefs.append(lasso.coef_)\n",
    "    \n",
    "ax = plt.gca()\n",
    "ax.plot(alphas*2, coefs)\n",
    "ax.set_xscale('log')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in the coefficient plot that depending on the choice of tuning parameter, some of the coefficients are exactly equal to zero. We now perform 10-fold cross-validation to choose the best alpha, refit the model, and compute the associated test error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104960.65853895503"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lassocv = LassoCV(alphas = None, cv = 10, max_iter = 100000, normalize = True)\n",
    "lassocv.fit(X_train, y_train)\n",
    "\n",
    "lasso.set_params(alpha=lassocv.alpha_)\n",
    "lasso.fit(X_train, y_train)\n",
    "mean_squared_error(y_test, lasso.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is substantially lower than the test set MSE of the null model and of least squares, and only a little worse than the test MSE of ridge regression with alpha chosen by cross-validation.\n",
    "\n",
    "However, the lasso has a substantial advantage over ridge regression in that the resulting coefficient estimates are sparse. Here we see that 13 of the 19 coefficient estimates are exactly zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AtBat           0.000000\n",
       "Hits            1.082446\n",
       "HmRun           0.000000\n",
       "Runs            0.000000\n",
       "RBI             0.000000\n",
       "Walks           2.906388\n",
       "Years           0.000000\n",
       "CAtBat          0.000000\n",
       "CHits           0.000000\n",
       "CHmRun          0.219367\n",
       "CRuns           0.000000\n",
       "CRBI            0.513975\n",
       "CWalks          0.000000\n",
       "PutOuts         0.368401\n",
       "Assists        -0.000000\n",
       "Errors         -0.000000\n",
       "League_N        0.000000\n",
       "Division_W    -89.064338\n",
       "NewLeague_N     0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some of the coefficients are now reduced to exactly zero.\n",
    "pd.Series(lasso.coef_, index=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
